{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAMES' AMES IOWA DATASET REGRESSION CHALLENGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "The objective of this project is to create a regression model based on the Ames Housing Dataset. This model will predict the price of a house at sale.\n",
    "\n",
    "The project will focus on sharpening the following skills:\n",
    "\n",
    "- Refining models over time\n",
    "- Use of train-test split\n",
    "- Use of Cross-validation\n",
    "- Using data with unknown values for the target to simulate the modeling process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#sklearn\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "test_data = pd.read_csv(\"./datasets/test.csv\")\n",
    "train_data = pd.read_csv(\"./datasets/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make into Dataframes\n",
    "df_test = pd.DataFrame(test_data)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train into Data frame\n",
    "df_train = pd.DataFrame(train_data)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Test data shape: \", df_test.shape)\n",
    "print(\"Train data shape: \", df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = df_train.isnull().sum() > 0\n",
    "df_train.loc[:,null_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot footage is a float with missing values, we can add impute some data for these\n",
    "df_train['Lot Frontage'].plot.hist();\n",
    "df_train['Lot Frontage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train['Lot Frontage'].fillna(69.1, inplace=True)\n",
    "\n",
    "df_train['Lot Frontage'].describe()\n",
    "# Does not affect stats greatly by using mean for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Lot Frontage'].fillna(69.6, inplace=True)\n",
    "df_test['Lot Frontage'].describe()\n",
    "# Does not affect stats greatly by using mean for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Mas Vnr Area'].fillna(0.0, inplace=True)\n",
    "# For another float with missing data, set values to 0, same as if it there was None\n",
    "# Only 30 homes like this so won't make a huge impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a closer look at the missing basement data\n",
    "# Will fill in missing categorical data to NA since bsmt sq ft is equal to 0\n",
    "bsmt_list_cat = ['Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin Type 2']\n",
    "df_train.fillna({x:'NA' for x in bsmt_list_cat}, inplace= True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pool is the highest null still\n",
    "# All null quality are just have no pool so will set to NA\n",
    "df_train['Pool QC'].fillna('NA', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Misc Feature'].value_counts()\n",
    "# just a random feature that is categorical so will set to NA\n",
    "df_train['Misc Feature'].fillna('NA', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If null value, will assume there is no alley\n",
    "df_train['Alley'].fillna('NA', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If null value, will assume there is no fence\n",
    "df_train['Fence'].fillna('NA', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('Fireplaces').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If null value, will assume there is no fireplace\n",
    "df_train['Fireplace Qu'].fillna('NA', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look to see if garage cars being at 0 corresponds with null values, looks like it does\n",
    "df_train.loc[df_train[\"Garage Cars\"] ==0 ,df_train.columns.str.contains('Garage')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fill the categorical missing data with NA, but year built is a float\n",
    "df_train['Garage Yr Blt'].describe()\n",
    "# uh oh we have a garage built in the future 2207, I think that really should be 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['Garage Yr Blt'] == 2207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.at[1699, 'Garage Yr Blt'] = 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['Garage Yr Blt'] == 2207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Garage Yr Blt'].plot.hist(bins=10)\n",
    "df_train['Year Built'].plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Garage Yr Blt'].fillna(df_train['Year Built'], inplace=True)\n",
    "\n",
    "# use year built when no garage present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in remaining categorical garage data with NA\n",
    "garage_list = list(df_train.loc[:,df_train.columns.str.contains('Garage')].columns)\n",
    "[df_train[x].fillna('NA', inplace= True) for x in garage_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing Masonary Veener Type with None\n",
    "df_train['Mas Vnr Type'].fillna('None', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.isnull().sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will just drop the remaining\n",
    "df_train.dropna(inplace= True)\n",
    "df_train.isnull().sum().sort_values(ascending= False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a close look at sale price\n",
    "df_train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "salepriceplot = sns.distplot(df_train['SalePrice'], kde=True,axlabel='Sale Price of Home', hue=\"Yr Sold\")\n",
    "sns.despine(ax=ax, offset=10)\n",
    "fig = salepriceplot.get_figure()\n",
    "fig.savefig(\"SalePrice.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parcel_id = df_train['PID']\n",
    "df_train.drop(columns=['Id', 'PID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate correlation with respect to price \n",
    "corr_col = df_train.corrwith(df_train['SalePrice']).sort_values(ascending=False)[1:].to_frame()\n",
    "fig2, ax2 = plt.subplots(figsize = (10,16))\n",
    "corrplot = sns.heatmap(corr_col, cmap= 'coolwarm', annot=True, vmin=-1, vmax=1);\n",
    "fig2 = corrplot.get_figure()\n",
    "fig2.savefig(\"CorrPlot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All EDA below here was done after initial runs to updated items I noticed after different iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check saleprice against highly correlated value\n",
    "sns.lmplot(x='Gr Liv Area', y='SalePrice', data=df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make column of sale price on logrithmic scale\n",
    "df_train['LogSalePrice'] = np.log1p(df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform data on log scale\n",
    "sns.lmplot(x='Gr Liv Area', y='LogSalePrice', data=df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the two outlier buildings that are clearly not normal to our training set\n",
    "df_train[df_train['Gr Liv Area'] > 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['Gr Liv Area'] < 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View other top correlate features\n",
    "sns.lmplot(x='Mas Vnr Area', y='LogSalePrice', data=df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the scale has changed, there are two more outlier on the low price side, let's take a look \n",
    "df_train[df_train['LogSalePrice'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will just remove them since one is agricultural\n",
    "df_train = df_train[df_train['LogSalePrice'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert ordinal categorical to values and numerical categories to strings\n",
    "ordered_rating_qual = { \"NA\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "\n",
    "# Not all of them but only a select few columns I want to convert\n",
    "qual_list = ['Exter Qual',\n",
    "             'Exter Cond',\n",
    "             'Bsmt Qual',\n",
    "             'Kitchen Qual',\n",
    "             'Fireplace Qu',\n",
    "             'Garage Qual'\n",
    "            ]\n",
    "\n",
    "for col in qual_list:\n",
    "    df_train[col] = df_train[col].map(ordered_rating_qual)\n",
    "    \n",
    "    \n",
    "\n",
    "#FUTURE REFERENCE METHOD FOR MAKING CATEGORICAL DICTIONARIES for .replace\n",
    "#labels = df_train['Exter Qual'].astype('category').cat.categories.tolist()\n",
    "#replace_map_comp = {'col' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in qual_list:\n",
    "    df_test[col] = df_test[col].map(ordered_rating_qual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['MS SubClass'] = df_train['MS SubClass'].astype(str)\n",
    "df_train['MS SubClass'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View counts by categories to see if there is enough \n",
    "df_train['MS Zoning'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a reference list of items that didn't have enough split on categocial info\n",
    "skip_list = ['Street', 'Utilities', 'Heating', 'Misc Feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup dummies on short list, doing all will not be helpful\n",
    "dummy_list = ['Neighborhood',\n",
    "              'Bldg Type',\n",
    "             'House Style',\n",
    "             'Roof Style',\n",
    "             'Exterior 1st',\n",
    "             'Exterior 2nd',\n",
    "             'Foundation',\n",
    "             'Functional',\n",
    "             'Garage Type',\n",
    "             'Garage Finish',\n",
    "              'Paved Drive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that doing get dummies on the test set will short change us on the columns so lets prep for that\n",
    "# DNR AGAIN\n",
    "for col in dummy_list:\n",
    "    df_test[col] = df_test[col].fillna(\"NA\")\n",
    "    test_values = sorted(list(df_test[col].unique()))\n",
    "    train_values = sorted(list(df_train[col].unique()))\n",
    "    categories = set(train_values + test_values)\n",
    "    df_test[col] = pd.Categorical(df_test[col], categories=categories)\n",
    "    df_train[col] = pd.Categorical(df_train[col], categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dummies on train dataset\n",
    "df_dud = df_train\n",
    "\n",
    "for col in dummy_list:\n",
    "    dummy = pd.get_dummies(df_train[col], columns=col, prefix=col)\n",
    "    df_dud = pd.concat([df_dud, dummy], axis=1)\n",
    "    \n",
    "df_dud.head()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to apply to test data too so columns match\n",
    "df_test_dud = df_test\n",
    "\n",
    "for col in dummy_list:\n",
    "    dummy = pd.get_dummies(df_test[col], columns=col, prefix=col)\n",
    "    df_test_dud = pd.concat([df_test_dud, dummy], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate correlation with respect to price with new data\n",
    "corr_col_2 = df_dud.corrwith(df_train['SalePrice']).sort_values(ascending=False)[2:].to_frame()\n",
    "fig2, ax2 = plt.subplots(figsize = (10,16))\n",
    "corrplot = sns.heatmap(corr_col_2.head(20), cmap= 'coolwarm', annot=True, vmin=-1, vmax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df_year)\n",
    "sns.barplot(x=index, y='Mean Sale Price by mo_yr', data=df_year);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in test numerical data for now\n",
    "\n",
    "df_test_junk = pd.DataFrame()\n",
    "for feature in num_columns:\n",
    "    df_test_junk[feature] = df_test[feature].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices\n",
    "num_columns = list(df_train._get_numeric_data().drop(['SalePrice', 'LogSalePrice'], axis=1).columns)\n",
    "cat_columns = list(df_train.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "ss = StandardScaler() # Instantiate Standard Scaler\n",
    "df_scaled = ss.fit_transform(df_train[num_columns]) \n",
    "df_test_scaled = ss.fit_transform(df_test_junk[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print shapes to confirm\n",
    "df_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW STEPS AFTER Dummies\n",
    "num_columns_2 = list(df_dud._get_numeric_data().drop(['SalePrice', 'LogSalePrice'], axis=1).columns)\n",
    "cat_columns_2 = list(df_dud.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for feature in num_columns:\n",
    "    df_test_dud[feature] = df_test_dud[feature].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "ss = StandardScaler() # Instantiate Standard Scaler\n",
    "df_scaled = ss.fit_transform(df_dud[num_columns]) \n",
    "df_test_scaled = ss.fit_transform(df_test_dud[num_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools ands Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_dump(y,yhat):\n",
    "    var_score = metrics.explained_variance_score(y,yhat)\n",
    "    mae = metrics.mean_absolute_error(y,yhat)\n",
    "    mse = metrics.mean_squared_error(y, yhat)\n",
    "    msle = metrics.mean_squared_log_error(y, yhat)\n",
    "    medae = metrics.median_absolute_error(y, yhat)\n",
    "    r2_score = metrics.r2_score(y, yhat)\n",
    "    results = {\"explained_variance_score\": var_score,\n",
    "                            \"mean_absolute_error\": mae,\n",
    "                            \"mean_squared_error\": mse,\n",
    "                            \"mean_squared_log_error\": msle,\n",
    "                            \"median_absolute_error\": medae,\n",
    "                            \"r2_score\": r2_score}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net 1 - A New Regression\n",
    "enet_alphas = np.arange(0.01, 1.0, 0.005)\n",
    "enet_ratio = 0.5\n",
    "enet_model = ElasticNetCV(alphas=enet_alphas, l1_ratio=enet_ratio, cv=5)\n",
    "enet_model = enet_model.fit(df_scaled, df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(enet_model, df_scaled, df_train['SalePrice'], cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_enet = enet_model.predict(df_test_junk[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net 2 - The GridSearch Strikes Back\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create train and test sets\n",
    "X = df_scaled\n",
    "y = df_train['LogSalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.7, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 20)\n",
    "alphas = np.logspace(0, 5, 100)\n",
    "param_grid = {'l1_ratio': l1_space,\n",
    "             'alpha': alphas}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gs = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gs_results = gs.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gs_results.predict(X_test)\n",
    "r2 = gs_results.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "gs_results = gs.fit(X_train, y_train)\n",
    "print(\"l1 ratio: {}\".format(gs_results.best_params_))\n",
    "print(\"R squared: {}\".format(r2))\n",
    "print(\"MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Iteration 1\n",
    "Setup Basic Elastic Net using numerical feature\n",
    "\n",
    "RESULTS: Iteration 1 high high MSE, need to optimize hyperparameters\n",
    "\n",
    "Will try again with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 2\n",
    "Using Grid Search\n",
    "\n",
    "RESULTS: Iteration 2 did not do better and actually scored worse than the non Grid search version.\n",
    "\n",
    "    l1 ratio: {'alpha': 1.5885651294280527, 'l1_ratio': 0.24137931034482757}\n",
    "    R squared: 0.7629059164812311\n",
    "    MSE: 1508666865.9788153\n",
    "\n",
    "Will try again with Log scaled SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 3\n",
    "Using Grid Search and LogSalePrice\n",
    "RESULTS: Iteration 3 did much better on R2, \n",
    "\n",
    "    l1 ratio: {'alpha': 1.0, 'l1_ratio': 0.0}\n",
    "    R squared: 0.8467819092554696\n",
    "    MSE: 0.025730495020561128 (Error is not one for one since we transformed the target)\n",
    "\n",
    "Will try again with dummies on select categorical columns and updating categorical with ordinal direction to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 4\n",
    "Using Grid Search and LogSalePrice and Dummies added and Ordinal Update\n",
    "RESULTS: Iteration 4 did much better on R^2, and we reduced our error by a substantial amount\n",
    "\n",
    "    l1 ratio: {'alpha': 1.0, 'l1_ratio': 0.0}\n",
    "    R squared: 0.8860361852037677\n",
    "    MSE: 0.01826427918929721\n",
    "\n",
    "Will try again with ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Run 1 - test data input as compared to other Kaggle submissions and where we could be heading\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "gs = GridSearchCV(RandomForestRegressor(), param_grid= params)\n",
    "gs.fit(df_train[num_columns], df_train['SalePrice'])\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Creation and Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## All interiations will be done above and this section will be for creating submissions only ##########\n",
    "#Set Predictions to be added to test data for Kaggle\n",
    "#predictions_enet = enet_model.predict(df_test_junk[num_columns])\n",
    "predictions_gs = gs_results.predict(df_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_from_log = np.expm1(predictions_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_junk['SalePrice'] = predictions_from_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_junk['Id'] = df_test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV for Kaggle submission\n",
    "df_test_junk[['Id', 'SalePrice']].to_csv(\"submission_loggridsearchdummies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
